import argparse
import json
import pandas as pd

EVAL_PROMPT='''\
You are an impartial grader of answers generated by an assistant.\n
Given the query and reference answer, determine if the assistant answer is correct, missing info, or incorrect.\n
Give score 1 if assistant answer is correct, 0 if missing info, -1 if incorrect.\n
Give reasons for your grading.\n
QUERY: {query}\n
REFERENCE ANSWER: {ref}\n
ASSISTANT ANSWER: {answer}\n
You MUST output in the following json format. {{"score": <your grading here>, "reason": <your reason here>}}\n
YOUR GRADING:\n
'''

# read the sampled data
def read_result_data(input_file):
    df = pd.read_json(input_file, lines=True)
    return df["query"].tolist(), df['answer'].tolist()

# for each query, get the candidate docs from the doc jsonl file
def get_ref_answer(query, df):
    ref = df[df["query"] == query]["answer"].values[0]
    # print(ref)
    return ref


def run_openai_api(args, query, answer, ref):
    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(
        model=args.model,
        temperature=0,
        max_tokens=args.max_new_tokens,
        timeout=None,
        max_retries=2,
        # api_key="...",
        # base_url="...",
        # organization="...",
        # other params...
    )

    messages = [
        ("human", EVAL_PROMPT.format(query=query, ref=ref, answer=answer)),
    ]
    res = llm.invoke(messages)
    # print(res.content)
    return res.content



def run_opea_llm_endpoint():
    # run opea llm endpoint
    pass

def parse_llm_output(output):
    output = json.loads(output)
    score = output['score']
    reason = output['reason']
    return score, reason
    
# for each candidate doc, generate a relevance score using LLM
def generate_scores(args, query, answer, ref):
    if args.use_openai_api:
        res = run_openai_api(args, query, answer, ref)
    elif args.use_opea_llm_endpoint:
        pass
    else:
        raise ValueError("ONLY OpenAI API or OPEA LLM endpoint are supported!")
    score, reason = parse_llm_output(res)
    return score, reason

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--result_file', type=str, default=None)
    parser.add_argument('--ref_file', type=str, default=None)
    parser.add_argument('--use_openai_api', action='store_true')
    parser.add_argument('--use_opea_llm_endpoint', action='store_true')
    parser.add_argument('--model', type=str, default='gpt-4o-2024-05-13', help="model for opanai API")
    parser.add_argument('--max_new_tokens', type=int, default=10)
    args = parser.parse_args()
    print(args)

    # read the result data
    queries, answers = read_result_data(args.result_file)
    # queries = queries[:2] # for debugging
    # answers = answers[:2] # for debugging
    # print(queries)

    # read the reference data
    df = pd.read_json(args.ref_file, lines=True)

    assert len(queries) == len(answers)
    assert len(queries) == df.shape[0]

    output = []
    score_list = []
    for query, answer in zip(queries, answers):
        ref = get_ref_answer(query, df)
        print("Query: {}\nAnswer: {}\nRef: {}".format(query, answer, ref))

        score, reason = generate_scores(args, query, answer, ref) 

        score_list.append(score)

        print("Score: {}\nReason: {}".format(score, reason))

        output.append({
            "query": query,
            "answer": answer,
            "ref_answer": ref,
            "score": score,
            "reason": reason
        })

        print('-'*50)
    
    # save the output to a file
    with open(args.result_file.replace('.jsonl', '_eval_{}.jsonl'.format(args.model)), 'w') as f:
        for d in output:
            f.write(json.dumps(d) + '\n')
        
    print("Average score: ", sum(score_list)/len(score_list))